---
title: "BikeShareAnalysis"
format: pdf
editor: visual
---

## EDA stuff

```{r}
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)

train <- vroom('/Users/laynelarson12/Documents/GitHub/STAT 348/BikeShare/train.csv') %>% select(-casual, -registered)
test <- vroom('/Users/laynelarson12/Documents/GitHub/STAT 348/BikeShare/test.csv') 


weather <- ggplot(data = train, aes(x = weather)) +
  geom_bar()

temp <- ggplot(data = train, aes(x = temp, y = count)) +
  geom_point() +
  geom_smooth(se = FALSE)

plot3 <- ggplot(data = train, aes(x = humidity, y = count)) +
  geom_point() +
  geom_smooth(se = FALSE)

plot4 <- ggplot(data = train, aes(x = season, y = count)) +
  geom_point()

(weather + temp) / (plot3 + plot4)
```

## Linear Model and stuff

```{r}

## Setup and Fit the Linear Regression Model
my_linear_model <- linear_reg() %>% 
set_engine("lm") %>%
set_mode("regression") %>% 
fit(formula=count~.-datetime, data=train)

## Generate Predictions Using Linear Model
bike_predictions <- predict(my_linear_model,
                            new_data=test) 
bike_predictions 


## Format the Predictions for Submission to Kaggle
kaggle_submission <- bike_predictions %>%
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle

## Write out the file
vroom_write(x=kaggle_submission, file="./LinearPreds.csv", delim=",")
```

## Recipe and Workflow stuff

```{r}

bike_recipe <- recipe(count ~ ., data = train) %>%
  step_log(all_outcomes(), base = exp(1), skip = TRUE) %>%
  step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
  step_mutate(weather = factor(weather)) %>%
  step_date(datetime, features = "dow") %>%
  step_time(datetime, features = "hour") %>%
  step_rm(datetime) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%  # <--- NEW
  step_zv(all_predictors()) %>%
  step_corr(all_predictors(), threshold = 0.5)
  
prepped_recipe <- prep(bike_recipe) 
baked_recipe <- bake(prepped_recipe, new_data=train)


## Workflow

bike_recipe_model <- linear_reg() %>%
  set_engine('lm') %>%
  set_mode("regression")

bike_workflow <- workflow() %>%
  add_recipe(bike_recipe) %>%
  add_model(bike_recipe_model) %>%
  fit(data=train)

bike_recipe_preds <- predict(bike_workflow, new_data = test) %>%
  mutate(.pred = exp(.pred))   # back-transform


## Format the Predictions for Submission to Kaggle
kaggle_submission_recipe1 <- bike_recipe_preds %>%
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle

## Write out the file
vroom_write(x=kaggle_submission_recipe1, file="./bike_recipe_preds.csv", delim=",")
```

## 

## Penalized Regression

```{r}

# --- Try multiple penalized models ---
penalties <- c(0.01, 0.1, 0.5, 1, 2)
mixtures  <- c(0, 0.25, 0.5, 0.75, 1)  # ridge -> lasso

results <- list()

for (p in penalties) {
  for (m in mixtures) {
    model <- linear_reg(penalty = p, mixture = m) %>%
      set_engine("glmnet") %>%
      set_mode("regression")

    wf <- workflow() %>%
      add_recipe(bike_recipe) %>%
      add_model(model) %>%
      fit(data = train)

    preds <- predict(wf, new_data = test) %>%
      mutate(.pred = exp(.pred)) # back-transform log(count)

    kaggle <- preds %>%
      bind_cols(test) %>%
      select(datetime, .pred) %>%
      rename(count = .pred) %>%
      mutate(count = pmax(0, count),
             datetime = as.character(format(datetime)))

    filename <- paste0("./glmnet_p", p, "_m", m, ".csv")
    vroom_write(kaggle, file = filename, delim = ",")

    results[[paste(p, m, sep = "_")]] <- kaggle
  }
}

```

## Tuning and Cross-Validation

```{r}

## Penalized Regression Model
preg_model <- linear_reg(penalty=tune(),
                         mixture=tune()) %>%
  set_engine("glmnet")

## Set Workflow
preg_wf <- workflow() %>%
  add_recipe(bike_recipe) %>%
  add_model(preg_model)

## Grid of values to tune over
grid_of_tuning_params <- grid_regular(penalty(),
                                      mixture(),
                                      levels=5)

## Split data for CV
folds <- vfold_cv(train, v = 5, repeats = 1)

## Run the CV
CV_results <- preg_wf %>%
  tune_grid(resamples = folds,
            grid = grid_of_tuning_params,
            metrics = metric_set(rmse))

## Plot results
collect_metrics(CV_results) %>%
  filter(.metric=="rmse") %>%
  ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
  geom_line()

## Find best tuning parameters
bestTune <- CV_results %>%
  select_best(metric="rmse")

## Finalize wf and fit it
final_wf <- preg_wf %>%
  finalize_workflow(bestTune) %>%
  fit(data=train)

## Predict
CV_preds <- final_wf %>%
  predict(new_data = test)

## Output to kaggle
kaggle_submission <- CV_preds %>%
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle

## Write out the file
vroom_write(x=kaggle_submission, file="./bike_recipe_preds.csv", delim=",")

```

## Regression Trees

```{r}

## tree model
tree_mod <- decision_tree(tree_depth = tune(),
                          cost_complexity = tune(),
                          min_n=tune()) %>% #Type of model
  set_engine("rpart") %>% # What R function to use
  set_mode("regression")

## Create a workflow with model & recipe
tree_wf <- workflow() %>%
  add_recipe(bike_recipe) %>%
  add_model(tree_mod)

## Set up grid of tuning values
grid_of_tree_params <- grid_regular(tree_depth(),
                                      cost_complexity(),
                                      min_n(),
                                      levels=5)

## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats = 1)

## Run the CV
CV_tree_results <- tree_wf %>%
  tune_grid(resamples = folds,
            grid = grid_of_tree_params,
            metrics = metric_set(rmse))

## Plot results
collect_metrics(CV_results) %>%
  filter(.metric=="rmse") %>%
  ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
  geom_line()

## Find best tuning parameters
bestTreeTune <- CV_tree_results %>%
  select_best(metric="rmse")


## Finalize workflow and Predict
final_wf <- tree_wf %>%
  finalize_workflow(bestTreeTune) %>%
  fit(data=train)

CV_tree_preds <- final_wf %>%
  predict(new_data = test)

## Output to kaggle
kaggle_submission <- CV_tree_preds %>%
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(count = exp(count)) %>%
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle

## Write out the file
vroom_write(x=kaggle_submission, file="./bike_recipe_preds.csv", delim=",")
```

## Random Forest

```{r}
library(rpart)
library(tidymodels)
forest_mod <- rand_forest(mtry = tune(),
                          min_n = tune(),
                          trees = 1000) %>%
  set_engine("ranger") %>%
  set_mode("regression")

forest_wf <- workflow() %>%
  add_recipe(bike_recipe) %>%
  add_model(forest_mod)

## Set up grid of tuning values
grid_of_tuning_params <- grid_regular(mtry(range=c(1,ncol(baked_recipe)-1)),
                                      min_n())

## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats = 1)

## Run the CV
CV_forest_results <- forest_wf %>%
  tune_grid(resamples = folds,
            grid = grid_of_tuning_params,
            metrics = metric_set(rmse))

## Find best tuning parameters
bestForestTune <- CV_forest_results %>%
  select_best(metric="rmse")


## Finalize workflow and Predict
final_wf <- forest_wf %>%
  finalize_workflow(bestForestTune) %>%
  fit(data=train)

CV_forest_preds <- final_wf %>%
  predict(new_data = test)

## Output to kaggle
kaggle_submission <- CV_forest_preds %>%
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(count = exp(count)) %>%
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle

## Write out the file
vroom_write(x=kaggle_submission, file="./bike_recipe_preds.csv", delim=",")

```
